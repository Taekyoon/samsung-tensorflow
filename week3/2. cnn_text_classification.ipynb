{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network for Text Classification\n",
    "In this tutorial, we are going to implement a convolutional neural network to classify movie review dataset(positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from utils import preprocess \n",
    "from model import T\n",
    "from sklearn.cross_validation import train_test_split\n",
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_pos = open('data/polarity/pos.txt').readlines()\n",
    "x_neg = open('data/polarity/neg.txt').readlines()\n",
    "y_pos = np.ones(len(x_pos))\n",
    "y_neg = np.zeros(len(x_neg))\n",
    "y = np.concatenate([y_pos, y_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5331\n",
      "5331\n",
      "if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \n",
      "\n",
      "simplistic , silly and tedious . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print len(x_pos)\n",
    "print len(x_neg)\n",
    "print x_pos[3]\n",
    "print x_neg[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, mask, word_to_idx, seq_length, vocab_size = preprocess(x_pos+x_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9595, 58)\n",
      "(1067, 58)\n",
      "(9595,)\n",
      "(1067,)\n"
     ]
    }
   ],
   "source": [
    "# randomly shuffle data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "\n",
    "print x_train.shape\n",
    "print x_test.shape\n",
    "print y_train.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=TextCNN(batch_size=100, seq_length=58, num_class=2, vocab_size=18768, \n",
    "                 dim_emb=256, filter_sizes=[3,4,5], num_filters=[50,50,50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1] Step: [1/95] loss: [0.529008] accuracy: [0.800000]\n",
      "model/textcnn-1-1 saved\n",
      "Epoch: [1] Step: [11/95] loss: [0.493539] accuracy: [0.880000]\n",
      "Epoch: [1] Step: [21/95] loss: [0.498853] accuracy: [0.890000]\n",
      "Epoch: [1] Step: [31/95] loss: [0.497739] accuracy: [0.730000]\n",
      "Epoch: [1] Step: [41/95] loss: [0.451746] accuracy: [0.920000]\n",
      "Epoch: [1] Step: [51/95] loss: [0.495826] accuracy: [0.870000]\n",
      "Epoch: [1] Step: [61/95] loss: [0.455766] accuracy: [0.820000]\n",
      "Epoch: [1] Step: [71/95] loss: [0.404962] accuracy: [0.920000]\n",
      "Epoch: [1] Step: [81/95] loss: [0.364094] accuracy: [0.910000]\n",
      "Epoch: [1] Step: [91/95] loss: [0.397500] accuracy: [0.890000]\n",
      "Epoch: [2] Step: [1/95] loss: [0.388932] accuracy: [0.810000]\n",
      "model/textcnn-2-1 saved\n",
      "Epoch: [2] Step: [11/95] loss: [0.337945] accuracy: [0.930000]\n",
      "Epoch: [2] Step: [21/95] loss: [0.351719] accuracy: [0.930000]\n",
      "Epoch: [2] Step: [31/95] loss: [0.311224] accuracy: [0.950000]\n",
      "Epoch: [2] Step: [41/95] loss: [0.269772] accuracy: [0.980000]\n",
      "Epoch: [2] Step: [51/95] loss: [0.292684] accuracy: [0.970000]\n",
      "Epoch: [2] Step: [61/95] loss: [0.277797] accuracy: [0.930000]\n",
      "Epoch: [2] Step: [71/95] loss: [0.209753] accuracy: [1.000000]\n",
      "Epoch: [2] Step: [81/95] loss: [0.161598] accuracy: [1.000000]\n",
      "Epoch: [2] Step: [91/95] loss: [0.216631] accuracy: [0.990000]\n",
      "Test accuracy: 0.718\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 2\n",
    "batch_size = 100\n",
    "num_iter_per_epoch = x_train.shape[0] / batch_size\n",
    "log_path = 'log/'\n",
    "model_save_path = 'model/'\n",
    "\n",
    "if not os.path.exists(log_path):\n",
    "    os.makedirs(log_path)\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    # initialize parameters\n",
    "    tf.initialize_all_variables().run()\n",
    "    summary_writer = tf.train.SummaryWriter(logdir=log_path, graph=tf.get_default_graph())\n",
    "\n",
    "    for e in range(num_epoch):\n",
    "        for i in range(num_iter_per_epoch):\n",
    "            # train the discriminator\n",
    "            x_batch = x_train[i*batch_size:(i+1)*batch_size]\n",
    "            y_batch = y_train[i*batch_size:(i+1)*batch_size]\n",
    "            feed_dict = {model.x: x_batch, model.y: y_batch}\n",
    "            sess.run(model.train_op, feed_dict)\n",
    "\n",
    "            \n",
    "\n",
    "            if i % 10 == 0:\n",
    "                summary, loss, acc = sess.run([model.summary_op, model.loss, model.accuracy], feed_dict)\n",
    "                summary_writer.add_summary(summary, e*num_iter_per_epoch + i)\n",
    "                print ('Epoch: [%d] Step: [%d/%d] loss: [%.6f] accuracy: [%.6f]' %(e+1, i+1, num_iter_per_epoch, loss, acc))\n",
    "\n",
    "            if i % 500 == 0:  \n",
    "                model.saver.save(sess, os.path.join(model_save_path, 'textcnn-%d' %(e+1)), global_step=i+1) \n",
    "                print ('model/textcnn-%d-%d saved' %(e+1, i+1))\n",
    "                \n",
    "                \n",
    "    \n",
    "    num_iter_per_epoch = int(x_test.shape[0] / batch_size)\n",
    "    test_accuracy = 0.0\n",
    "    for i in range(num_iter_per_epoch):\n",
    "        x_batch = x_test[i*batch_size:(i+1)*batch_size]\n",
    "        y_batch = y_test[i*batch_size:(i+1)*batch_size]\n",
    "        acc = sess.run(model.accuracy, feed_dict={model.x: x_batch, model.y: y_batch})\n",
    "        test_accuracy += acc\n",
    "\n",
    "    print (\"Test accuracy: %.3f\" %(test_accuracy/num_iter_per_epoch))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
