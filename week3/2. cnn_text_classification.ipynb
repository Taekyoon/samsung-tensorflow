{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network for Text Classification\n",
    "In this tutorial, we are going to implement a convolutional neural network to classify movie review dataset(positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from utils import *\n",
    "from sklearn.cross_validation import train_test_split\n",
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_pos = open('data/polarity/pos.txt').readlines()\n",
    "x_neg = open('data/polarity/neg.txt').readlines()\n",
    "y_pos = np.ones(len(x_pos))\n",
    "y_neg = np.zeros(len(x_neg))\n",
    "y = np.concatenate([y_pos, y_neg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, mask, word_to_idx, seq_length, vocab_size = preprocess(x_pos+x_neg)\n",
    "\n",
    "# randomly shuffle data\n",
    "np.random.seed(10)\n",
    "random_idx = np.random.permutation(len(y))\n",
    "x = x[random_idx]\n",
    "y = y[random_idx]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \n",
    "    def __init__(self, batch_size=100, seq_length=58, num_class=2, vocab_size=18768, \n",
    "                 dim_emb=128, filter_sizes=[3,4,5], num_filters=[100,100,100]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            seq_length: maximum sequence length\n",
    "            num_class: number of class; default is 2 (postive or negative)\n",
    "            vocab_size: vocabulary size; number of different words\n",
    "            dim_emb: embedding size\n",
    "            filter_sizes: list for filter size; e.g [3, 4, 5]\n",
    "            num_filters: list for number of filter; e.g [128, 128, 128]    \n",
    "        \"\"\"\n",
    "        \n",
    "        self.x = tf.placeholder(tf.int32, [None, seq_length], name='x')\n",
    "        self.y = tf.placeholder(tf.int64, [None], name='y')\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope('embedding_layer'):\n",
    "            w = tf.get_variable('w', shape=[vocab_size, dim_emb], initializer=tf.random_uniform_initializer(-1, 1))\n",
    "            x_embed = tf.nn.embedding_lookup(w, self.x)    # (batch_size, seq_length, dim_emb)\n",
    "            x_embed = tf.expand_dims(x_embed, 3)          # (batch_size, seq_length, dim_emb, 1)\n",
    "            \n",
    "        pooled_outputs = []\n",
    "        num_total_filter = 0\n",
    "        for i, (f_s, n_f) in enumerate(zip(filter_sizes, num_filters)):\n",
    "            num_total_filter += n_f\n",
    "            \n",
    "            with tf.variable_scope('conv_maxpool_%d' %(i+1)):\n",
    "                w = tf.get_variable('w', shape=[f_s, dim_emb, 1, n_f], initializer=tf.contrib.layers.xavier_initializer())\n",
    "                b = tf.get_variable('b', shape=[n_f], initializer=tf.constant_initializer(0.0))\n",
    "                \n",
    "                conv = tf.nn.conv2d(x_embed, w, strides=[1, 1, 1, 1], padding='VALID') + b   # (batch_size, seq_length - filter_size + 1, 1, num_filter)\n",
    "                relu = tf.nn.relu(conv)\n",
    "                pooled = tf.nn.max_pool(relu, [1, seq_length - f_s + 1, 1, 1], [1, 1, 1, 1], padding='VALID')\n",
    "                pooled_outputs.append(pooled)  # (number of diffent filter) @ [batch_size, 1, 1, num_filter]\n",
    "        \n",
    "        pooled = tf.concat(3, pooled_outputs)\n",
    "        pooled = tf.reshape(pooled, [batch_size, -1])\n",
    "        \n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            pooled = tf.nn.dropout(pooled, self.dropout_keep_prob)\n",
    "        \n",
    "        with tf.variable_scope('output_layer'):\n",
    "    \n",
    "            w = tf.get_variable('w', shape=[num_total_filter, num_class], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.get_variable('b', shape=[num_class], initializer=tf.constant_initializer(0.0))\n",
    "            \n",
    "            out = tf.matmul(pooled, w) + b    # (batch_size, num_class)\n",
    "        \n",
    "        with tf.name_scope('optimizer'):\n",
    "            self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(out, self.y))\n",
    "            self.train_op = tf.train.AdamOptimizer(0.001, beta1=0.5).minimize(self.loss)        \n",
    "        \n",
    "        with tf.name_scope('evaluation'):\n",
    "            self.pred = tf.arg_max(out, 1)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.pred, self.y), tf.float32))\n",
    "            \n",
    "            \n",
    "        with tf.name_scope('summary'):\n",
    "            tf.scalar_summary('batch_loss', self.loss)\n",
    "            tf.scalar_summary('accuracy', self.accuracy)\n",
    "            for var in tf.trainable_variables():\n",
    "                tf.histogram_summary(var.op.name, var)\n",
    "            \n",
    "            self.summary_op = tf.merge_all_summaries() \n",
    "        self.saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=TextCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1] Step: [1/95] loss: [0.498247] accuracy: [1.000000]\n",
      "model/textcnn-1-1 saved\n",
      "Epoch: [1] Step: [11/95] loss: [0.527520] accuracy: [0.860000]\n",
      "Epoch: [1] Step: [21/95] loss: [0.543972] accuracy: [0.800000]\n",
      "Epoch: [1] Step: [31/95] loss: [0.515876] accuracy: [0.800000]\n",
      "Epoch: [1] Step: [41/95] loss: [0.487641] accuracy: [0.880000]\n",
      "Epoch: [1] Step: [51/95] loss: [0.528076] accuracy: [0.830000]\n",
      "Epoch: [1] Step: [61/95] loss: [0.495067] accuracy: [0.830000]\n",
      "Epoch: [1] Step: [71/95] loss: [0.442330] accuracy: [0.880000]\n",
      "Epoch: [1] Step: [81/95] loss: [0.396540] accuracy: [0.930000]\n",
      "Epoch: [1] Step: [91/95] loss: [0.456583] accuracy: [0.870000]\n",
      "Epoch: [2] Step: [1/95] loss: [0.383526] accuracy: [0.890000]\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 2\n",
    "batch_size = 100\n",
    "num_iter_per_epoch = x_train.shape[0] / batch_size\n",
    "log_path = 'log/'\n",
    "model_save_path = 'model/'\n",
    "\n",
    "if not os.path.exists(log_path):\n",
    "    os.makedirs(log_path)\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    # initialize parameters\n",
    "    tf.initialize_all_variables().run()\n",
    "    summary_writer = tf.train.SummaryWriter(logdir=log_path, graph=tf.get_default_graph())\n",
    "\n",
    "    for e in range(num_epoch):\n",
    "        for i in range(num_iter_per_epoch):\n",
    "            # train the discriminator\n",
    "            x_batch = x_train[i*batch_size:(i+1)*batch_size]\n",
    "            y_batch = y_train[i*batch_size:(i+1)*batch_size]\n",
    "            keep_prob = 1.0\n",
    "            feed_dict = {model.x: x_batch, model.y: y_batch, model.dropout_keep_prob: keep_prob}\n",
    "            sess.run(model.train_op, feed_dict)\n",
    "\n",
    "            \n",
    "\n",
    "            if i % 10 == 0:\n",
    "                summary, loss, acc = sess.run([model.summary_op, model.loss, model.accuracy], feed_dict)\n",
    "                summary_writer.add_summary(summary, e*num_iter_per_epoch + i)\n",
    "                print ('Epoch: [%d] Step: [%d/%d] loss: [%.6f] accuracy: [%.6f]' %(e+1, i+1, num_iter_per_epoch, loss, acc))\n",
    "\n",
    "            if i % 500 == 0:  \n",
    "                model.saver.save(sess, os.path.join(model_save_path, 'textcnn-%d' %(e+1)), global_step=i+1) \n",
    "                print ('model/textcnn-%d-%d saved' %(e+1, i+1))\n",
    "                \n",
    "                \n",
    "    \n",
    "    num_iter_per_epoch = int(x_test.shape[0] / batch_size)\n",
    "    test_accuracy = 0.0\n",
    "    for i in range(num_iter_per_epoch):\n",
    "        x_batch = x_test[i*batch_size:(i+1)*batch_size]\n",
    "        y_batch = y_test[i*batch_size:(i+1)*batch_size]\n",
    "        keep_prob = 1.0\n",
    "        acc = sess.run(model.accuracy, feed_dict={model.x: x_batch, model.y: y_batch, model.dropout_keep_prob: keep_prob})\n",
    "        test_accuracy += acc\n",
    "\n",
    "    print (\"Test accuracy: %.3f\" %(test_accuracy/num_iter_per_epoch))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
