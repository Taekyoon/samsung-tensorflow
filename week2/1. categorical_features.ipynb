{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network with Categorical Features\n",
    "In this tutorial, we are going to implement a neural network using continuous and categorical features to classify Titanic Kaggle dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "We are going to  use the Titanic Kaggle data to predict whether or not the passenger will survive based on certain attributes like age, gender, passenger class and the fare they paid etc. For more information on this data set check out here at [Kaggle](https://www.kaggle.com/c/titanic/data).\n",
    "First off we’re going to define all of our columns as 'continuous' or 'categorical'.\n",
    "* <b>Continuous columns</b> — any numerical value in a continuous range. Pretty much if it is a numerical representation like money, or age.\n",
    "* <b>Categorical columns</b> — part of a finite set. Like male or female, or even what country someone is from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/titanic.csv')\n",
    "continuous_vars = ['Age', 'SibSp', 'Parch', 'Fare']\n",
    "categorical_vars = ['Pclass', 'Gender', 'Embarked']\n",
    "\n",
    "x = data[continuous_vars + categorical_vars]\n",
    "y = np.asarray(data.pop('Survived'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim_emb = 8\n",
    "dim_in = len(continuous_vars) + dim_emb * len(categorical_vars)\n",
    "dim_h = 100\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Process categorical variables into ids.\n",
    "x_train = x_train.copy()\n",
    "x_test = x_test.copy()\n",
    "categorical_var_encoders = {}\n",
    "for var in categorical_vars:\n",
    "    le = LabelEncoder().fit(x_train[var])\n",
    "    x_train[var + '_id'] = le.transform(x_train[var])\n",
    "    x_test[var + '_id'] = le.transform(x_test[var])\n",
    "    x_train.pop(var)\n",
    "    x_test.pop(var)\n",
    "    categorical_var_encoders[var] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(x, dim_in, dim_out, name):\n",
    "    with tf.variable_scope(name):\n",
    "        # create variables\n",
    "        w = tf.get_variable('w', shape=[dim_in, dim_out], initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))\n",
    "        b = tf.get_variable('b', shape=[dim_out])\n",
    "\n",
    "        # create operations\n",
    "        out = tf.matmul(x, w) + b\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def neural_network(x, dim_in=dim_in, dim_h=dim_h, dim_out=n_classes):\n",
    "    # append continuous variables\n",
    "    final_features = []\n",
    "    final_features.append(x[:, :4])\n",
    "    \n",
    "    # embed categorical variables into distributed representation\n",
    "    for i, var in enumerate(categorical_vars):\n",
    "        feature = learn.ops.categorical_variable(tf.cast(x[:, i+4], tf.int64), \n",
    "                                                 len(categorical_var_encoders[var].classes_),\n",
    "                                                 embedding_size=dim_emb, \n",
    "                                                 name=var)\n",
    "        final_features.append(feature)\n",
    "\n",
    "    final_features = tf.concat(1, final_features)\n",
    "\n",
    "\n",
    "    # lst hidden layer with ReLU\n",
    "    h1 = fully_connected(final_features, dim_in, dim_h, 'h1')\n",
    "    h1 = tf.nn.relu(h1)\n",
    "\n",
    "    # 2nd hidden layer with ReLU\n",
    "    h2 = fully_connected(h1, dim_h, dim_h, 'h2')\n",
    "    h2 = tf.nn.relu(h2)\n",
    "\n",
    "    # output layer with linear\n",
    "    out = fully_connected(h2, dim_h, dim_out, 'out')\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 7])\n",
    "y = tf.placeholder(tf.int64, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construct model with default value\n",
    "out = neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = np.asarray(x_train)\n",
    "x_test = np.asarray(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pclass/Pclass_embeddings (3, 8)\n",
      "Gender/Gender_embeddings (2, 8)\n",
      "Embarked/Embarked_embeddings (3, 8)\n",
      "h1/w (28, 100)\n",
      "h1/b (100,)\n",
      "h2/w (100, 100)\n",
      "h2/b (100,)\n",
      "out/w (100, 2)\n",
      "out/b (2,)\n"
     ]
    }
   ],
   "source": [
    "for var in tf.all_variables():\n",
    "    print var.op.name, var.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(out, y))\n",
    "train_op = tf.train.RMSPropOptimizer(learning_rate=0.003).minimize(loss)\n",
    "\n",
    "# Test model\n",
    "pred = tf.argmax(out, 1)\n",
    "\n",
    "correct_pred = tf.equal(pred, y)\n",
    "incorrect_pred = tf.not_equal(pred, y)\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.769\n",
      "Epoch 101, Loss: 0.314\n",
      "Epoch 201, Loss: 0.265\n",
      "Epoch 301, Loss: 0.244\n",
      "Epoch 401, Loss: 0.241\n",
      "Epoch 501, Loss: 0.253\n",
      "Epoch 601, Loss: 0.263\n",
      "Epoch 701, Loss: 0.201\n",
      "Epoch 801, Loss: 0.231\n",
      "Epoch 901, Loss: 0.192\n",
      "Finished training!\n",
      "\n",
      "Train accuracy: 0.914326\n",
      "\n",
      "Test accuracy: 0.804469\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "num_epochs = 500\n",
    "\n",
    "# launch the graph\n",
    "with tf.Session() as sess:\n",
    "    # initialize tensor variables\n",
    "    tf.initialize_all_variables().run()\n",
    "    # training cycle\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_loss = 0.\n",
    "        n_iters_per_epoch = int(x_train.shape[0]/ batch_size)\n",
    "        # loop over all batches\n",
    "        for i in range(n_iters_per_epoch):\n",
    "            x_batch, y_batch = x_train[i*batch_size:(i+1)*batch_size], y_train[i*batch_size:(i+1)*batch_size]\n",
    "            # run optimization op (backprop) and loss op (to get loss value)\n",
    "            _, c = sess.run([train_op, loss], feed_dict={x: x_batch, y: y_batch})\n",
    "            # compute average loss\n",
    "            avg_loss += c / n_iters_per_epoch\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print \"Epoch %d, Loss: %.3f\"% (epoch+1, avg_loss)\n",
    "    print \"Finished training!\"\n",
    "    \n",
    "    print \"\\nTrain accuracy:\", sess.run(accuracy, {x: x_train, y: y_train})\n",
    "    \n",
    "    print \"\\nTest accuracy:\", sess.run(accuracy, {x: x_test, y: y_test})"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
