{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network with Categorical Features\n",
    "In this tutorial, we are going to implement a neural network using continuous and categorical features to classify Titanic Kaggle dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "We are going to  use the Titanic Kaggle data to predict whether or not the passenger will survive based on certain attributes such as age, gender, passenger class and the fare they paid etc. For more information on this data set check out here at [Kaggle](https://www.kaggle.com/c/titanic/data).\n",
    "First, we are going to define all of our featyres as 'continuous' or 'categorical'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/titanic.csv')\n",
    "continuous_features = ['Age', 'Parch', 'SibSp', 'Fare']\n",
    "categorical_features = ['Pclass', 'Gender', 'Embarked']\n",
    "\n",
    "x = data[continuous_features + categorical_features]\n",
    "y = np.asarray(data.pop('Survived'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim_emb = 8\n",
    "dim_in = len(continuous_features) + dim_emb * len(categorical_features)\n",
    "dim_h = 64\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Process categorical variables into ids.\n",
    "x_train = x_train.copy()\n",
    "x_test = x_test.copy()\n",
    "num_class = {}\n",
    "for var in categorical_features:\n",
    "    le = LabelEncoder().fit(x_train[var])\n",
    "    x_train[var + '_id'] = le.transform(x_train[var])\n",
    "    x_test[var + '_id'] = le.transform(x_test[var])\n",
    "    x_train.pop(var)\n",
    "    x_test.pop(var)\n",
    "    num_class[var] = len(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and build model\n",
    "we will develop a feed forward neural network with embedding features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(x, dim_in, dim_out, name):\n",
    "    with tf.variable_scope(name):\n",
    "        # create variables\n",
    "        w = tf.get_variable('w', shape=[dim_in, dim_out], initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))\n",
    "        b = tf.get_variable('b', shape=[dim_out])\n",
    "\n",
    "        # create operations\n",
    "        out = tf.matmul(x, w) + b\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding(x, num_class, dim_emb, name):\n",
    "    # data type casting\n",
    "    x = tf.cast(x, tf.int64)\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        # embedding matrix\n",
    "        w = tf.get_variable('w', shape=[num_class, dim_emb], initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))\n",
    "        \n",
    "        out = tf.nn.embedding_lookup(w, x)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def neural_network(x, dim_in=dim_in, dim_h=dim_h, dim_out=n_classes):\n",
    "    # append continuous variables\n",
    "    all_features = []\n",
    "    \n",
    "    # append continuous features (Age, SibSp, Parch, Fare)\n",
    "    all_features.append(x[:, :len(continuous_features)])     \n",
    "    \n",
    "    # embed categorical variables into continuoues vector space\n",
    "    for i, var in enumerate(categorical_features):\n",
    "        feature = embedding(x[:, i+len(continuous_features)], num_class[var], dim_emb, name=var)\n",
    "        all_features.append(feature)\n",
    "    \n",
    "    all_features = tf.concat(1, all_features)\n",
    "\n",
    "    # lst hidden layer with ReLU\n",
    "    h1 = fully_connected(all_features, dim_in, dim_h, 'h1')\n",
    "    h1 = tf.nn.relu(h1)\n",
    "\n",
    "    # 2nd hidden layer with ReLU\n",
    "    h2 = fully_connected(h1, dim_h, dim_h, 'h2')\n",
    "    h2 = tf.nn.relu(h2)\n",
    "\n",
    "    # output layer with linear\n",
    "    out = fully_connected(h2, dim_h, dim_out, 'out')\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 7])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "# Construct model with default value\n",
    "out = neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(out, y))\n",
    "train_op = tf.train.RMSPropOptimizer(learning_rate=0.002).minimize(loss)\n",
    "\n",
    "# Test model\n",
    "pred = tf.argmax(out, 1)\n",
    "correct_pred = tf.equal(pred, y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pclass/w (3, 8)\n",
      "Gender/w (2, 8)\n",
      "Embarked/w (3, 8)\n",
      "h1/w (28, 64)\n",
      "h1/b (64,)\n",
      "h2/w (64, 64)\n",
      "h2/b (64,)\n",
      "out/w (64, 2)\n",
      "out/b (2,)\n"
     ]
    }
   ],
   "source": [
    "for var in tf.trainable_variables():\n",
    "    print var.op.name, var.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.886\n",
      "Epoch 101, Loss: 0.344\n",
      "Epoch 201, Loss: 0.296\n",
      "Epoch 301, Loss: 0.279\n",
      "Epoch 401, Loss: 0.261\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "num_epoch = 500\n",
    "num_ter_per_epoch = int(x_train.shape[0]/ batch_size)\n",
    "\n",
    "# convert pandas dataframe to numpy array \n",
    "x_train = np.asarray(x_train)\n",
    "x_test = np.asarray(x_test)\n",
    "\n",
    "# launch the graph\n",
    "with tf.Session() as sess:\n",
    "    # initialize tensor variables\n",
    "    tf.initialize_all_variables().run()\n",
    "    # training cycle\n",
    "    for e in range(num_epoch):\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        # loop over all batches\n",
    "        for i in range(num_ter_per_epoch):\n",
    "            x_batch, y_batch = x_train[i*batch_size:(i+1)*batch_size], y_train[i*batch_size:(i+1)*batch_size]\n",
    "            # run optimization op (backprop) and loss op (to get loss value)\n",
    "            _, c = sess.run([train_op, loss], feed_dict={x: x_batch, y: y_batch})\n",
    "            # compute average loss\n",
    "            avg_loss += c / num_ter_per_epoch\n",
    "        \n",
    "        if e % 100 == 0:\n",
    "            print \"Epoch %d, Loss: %.3f\"% (e+1, avg_loss)\n",
    "    print \"Finished training!\"\n",
    "    \n",
    "    print \"\\nTrain accuracy:\", sess.run(accuracy, {x: x_train, y: y_train})\n",
    "    \n",
    "    print \"\\nTest accuracy:\", sess.run(accuracy, {x: x_test, y: y_test})"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
